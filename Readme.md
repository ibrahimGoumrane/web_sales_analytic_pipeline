# Web Sales Analytics ETL Pipeline

> End-to-end data engineering project showcasing ETL pipelines, Apache Airflow orchestration, and web scraping from Jumia.ma

## Overview

This project demonstrates a production-ready ETL pipeline that extracts product and pricing data from Jumia.ma, transforms it into analytics-ready formats, and loads it into a PostgreSQL data warehouse. The entire pipeline is orchestrated with Apache Airflow and containerized with Docker for reliable, scheduled execution.

## Features

### ğŸ” Data Extraction

- Web scraping with BeautifulSoup and Requests
- Automatic category discovery and pagination handling
- Extracts 1000+ products per category from Jumia.ma
- Raw data storage in JSON/CSV format
- Intelligent retry logic and rate limiting

### ğŸ”„ Data Transformation

- Data cleaning with Pandas:
  - Price normalization (removes currency, handles decimals)
  - Discount percentage cleaning
  - Rating and review count standardization
  - Boolean conversion for official store badges
  - Date/time handling

### ğŸ’¾ Data Loading

- PostgreSQL data warehouse with two databases:
  - `airflow` â€” Airflow metadata
  - `sales_analytics` â€” Product data warehouse
- Automated database and schema creation
- Conflict handling with UNIQUE constraints
- Idempotent pipeline tasks for safe re-runs

### âš™ï¸ Workflow Orchestration

Fully orchestrated with Apache Airflow:

**Main ETL DAG (`jumia_daily_etl`):**

- `scrape_jumia` â€” Extract products from Jumia.ma
- `transform_jumia` â€” Clean and normalize data
- `load_jumia` â€” Load to PostgreSQL warehouse
- Daily scheduling with retry logic
- Task dependency management

**Analytics DAG (`analytics_report_param`):**

- `generate_report` â€” Generate comprehensive analytics reports
- Manual/API triggered with configurable parameters
- Produces CSV reports and PNG visualizations
- Includes: category stats, top products, discounts, price/rating analysis

## Tech Stack

| Component            | Technology               |
| -------------------- | ------------------------ |
| **Orchestration**    | Apache Airflow 2.8.1     |
| **Web Scraping**     | BeautifulSoup4, Requests |
| **Database**         | PostgreSQL 13            |
| **Data Processing**  | Pandas                   |
| **Containerization** | Docker & Docker Compose  |
| **Storage**          | Local filesystem         |
| **Python**           | 3.8+                     |

## Project Structure

```
web_sales_analytic_pipeline/
â”‚
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ sales_etl_dag.py          # Main DAG definition (scrape, transform, load)
â”‚
â”œâ”€â”€ scraping/
â”‚   â”œâ”€â”€ base.py                       # Abstract base scraper class
â”‚   â”œâ”€â”€ main.py                       # Scraper orchestrator
â”‚   â”œâ”€â”€ utils.py                      # URL handling utilities
â”‚   â””â”€â”€ jumia/
â”‚       â””â”€â”€ main.py                   # Jumia scraper implementation
â”‚
â”œâ”€â”€ transform/
â”‚   â”œâ”€â”€ base.py                       # Abstract base transformer
â”‚   â”œâ”€â”€ main.py                       # Transform orchestrator
â”‚   â””â”€â”€ jumia/
â”‚       â””â”€â”€ main.py                   # Jumia data cleaner
â”‚
â”œâ”€â”€ load/
â”‚   â””â”€â”€ load_postgres.py              # PostgreSQL loader with analytics queries
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ base.py                       # Analytics report generator
â”‚   â”œâ”€â”€ helpers.py                    # Plotting utility functions
â”‚   â”œâ”€â”€ main.py                       # Report orchestrator
â”‚   â””â”€â”€ __init__.py                   # Lazy import wrapper
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/jumia/                    # Raw scraped data (JSON/CSV)
â”‚   â”œâ”€â”€ processed/jumia/              # Cleaned data (CSV)
â”‚   â””â”€â”€ reports/report_YYYYMMDD/      # Generated analytics reports
â”‚       â”œâ”€â”€ *.csv                     # Category stats, top products, discounts, etc.
â”‚       â”œâ”€â”€ *.png                     # Visualizations and charts
â”‚       â””â”€â”€ REPORT_SUMMARY.txt        # Executive summary
â”‚
â”œâ”€â”€ html_scruture/
â”‚   â””â”€â”€ jumia/                        # HTML samples for scraper reference
â”‚
â”œâ”€â”€ logs/                             # Application logs
â”œâ”€â”€ docker-compose.yaml               # Multi-container orchestration
â”œâ”€â”€ Dockerfile                        # Custom Airflow image with plotting libs
â”œâ”€â”€ Makefile                          # Build automation commands
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ .env                              # Environment variables (not in Git)
â”œâ”€â”€ StartupDocs.md                    # Quick start guide
â”œâ”€â”€ Recomendations.md                 # Next steps and enhancements
â”œâ”€â”€ README_DB.md                      # Database interrogation guide (not in Git)
â””â”€â”€ README.md                         # This file
```

## Getting Started

### Prerequisites

- **Docker Desktop** 20.10+
- **Python** 3.8+
- **Make** (Windows: gnuwin32.sourceforge.net)
- **Git**

### Quick Start

1. **Clone the repository**

   ```bash
   git clone https://github.com/ibrahimGoumrane/web_sales_analytic_pipeline.git
   cd web_sales_analytic_pipeline
   ```

2. **Ensure Docker Desktop is running**

   ```bash
   docker --version
   docker ps
   ```

3. **Start the entire pipeline**

   ```bash
   make all
   ```

   This will:

   - Start PostgreSQL database
   - Initialize Airflow (creates admin user)
   - Start Airflow webserver and scheduler
   - Wait ~2-3 minutes for initialization

4. **Access Airflow UI**

   - URL: http://localhost:8085
   - Username: `admin`
   - Password: `admin`

5. **Run the pipeline**
   - Find `jumia_daily_etl` DAG in the UI
   - Toggle switch to enable
   - Click â–¶ï¸ Play â†’ Trigger DAG
   - Monitor execution (Green = Success)

### Verify Installation

```bash
# Check services are running
make ps

# View logs
make logs

# Check database
docker exec -it web_sales_analytic_pipeline-postgres-1 psql -U airflow -d sales_analytics
# Inside psql:
SELECT COUNT(*) FROM products WHERE website = 'jumia';
\q
```

### Available Commands

```bash
make help          # Show all available commands
make up            # Start all services
make down          # Stop all services
make restart       # Restart services
make logs          # View all logs
make clean         # Remove all data and reset
make ps            # Check container status
```

For detailed setup instructions, see [StartupDocs.md](StartupDocs.md).

## Data Output

### Database Schema

**Database:** `sales_analytics`  
**Table:** `products`

| Column            | Type          | Description            |
| ----------------- | ------------- | ---------------------- |
| id                | SERIAL        | Primary key            |
| website           | VARCHAR(50)   | Source website (jumia) |
| sku               | VARCHAR(100)  | Product SKU            |
| name              | TEXT          | Product name           |
| url               | TEXT          | Product URL            |
| current_price     | NUMERIC(10,2) | Current price          |
| old_price         | NUMERIC(10,2) | Original price         |
| discount          | NUMERIC(5,2)  | Discount percentage    |
| rating            | NUMERIC(3,2)  | Product rating (0-5)   |
| review_count      | INTEGER       | Number of reviews      |
| is_official_store | BOOLEAN       | Official store badge   |
| image_url         | TEXT          | Product image URL      |
| scraped_at        | TIMESTAMP     | Scraping timestamp     |
| created_at        | TIMESTAMP     | Record creation time   |

### File Outputs

- **`data/raw/jumia/`** â€” Raw scraped data (JSON/CSV)
- **`data/processed/jumia/`** â€” Cleaned product data (CSV)
- **`data/reports/report_YYYYMMDD/`** â€” Analytics reports and visualizations:
  - `category_statistics.csv` â€” Product counts, prices, ratings by category
  - `top_rated_products.csv` â€” Top 50 products by weighted score
  - `biggest_discounts.csv` â€” Top 100 products by discount percentage
  - `price_statistics.csv` â€” Price distribution metrics
  - `rating_statistics.csv` â€” Rating and review analysis
  - `store_performance.csv` â€” Official vs non-official store comparison
  - `daily_summary.csv` â€” Daily aggregated statistics
  - `*.png` â€” 6 visualization charts
  - `REPORT_SUMMARY.txt` â€” Executive summary
- **`logs/`** â€” Application logs

## Pipeline Execution

### Daily ETL DAG (`jumia_daily_etl`)

Runs daily at midnight with the following workflow:

1. **Scrape Jumia** (Task: `scrape_jumia`)

   - Discovers categories automatically
   - Scrapes 100 products per category
   - Saves raw data to `data/raw/jumia/`

2. **Transform Data** (Task: `transform_jumia`)

   - Cleans prices and percentages
   - Normalizes ratings and reviews
   - Saves to `data/processed/jumia/`

3. **Load to Database** (Task: `load_jumia`)
   - Inserts cleaned data into PostgreSQL
   - Handles duplicates with UNIQUE constraints
   - Creates `sales_analytics` database automatically

### Analytics Report DAG (`analytics_report_param`)

Manual/API-triggered report generation:

4. **Generate Reports** (Task: `generate_report`)
   - Loads data from PostgreSQL by date
   - Generates 7 CSV reports with statistics
   - Creates 6 PNG visualizations (matplotlib/seaborn)
   - Saves to `data/reports/report_YYYYMMDD/`
   - Trigger with JSON: `{"website": "jumia", "date": "YYYY-MM-DD"}`

## Skills Demonstrated

- âœ… **ETL Pipeline Design** â€” Complete Extract-Transform-Load workflow
- âœ… **Apache Airflow** â€” DAG creation, task dependencies, scheduling
- âœ… **Web Scraping** â€” BeautifulSoup, pagination, category discovery
- âœ… **Data Engineering** â€” Pandas transformations, data cleaning
- âœ… **Database Design** â€” PostgreSQL schema, indexing, constraints
- âœ… **Containerization** â€” Docker Compose multi-service orchestration
- âœ… **DevOps** â€” Makefile automation, environment management
- âœ… **Code Organization** â€” Abstract base classes, modular design

## Future Enhancements

See [Recommendations.md](Recommendations.md) for planned improvements:

- Report generation and analytics
- Data visualization dashboards
- Additional e-commerce site support
- Cloud deployment (AWS/GCP/Azure)
- API layer for data access

## Connection Details

**Airflow Web UI:**

- URL: http://localhost:8085
- Username: `admin`
- Password: `admin`

**PostgreSQL:**

- Host: `localhost`
- Port: `5432`
- User: `airflow`
- Password: `airflow`
- Databases: `airflow`, `sales_analytics`

## Troubleshooting

Common issues and solutions:

| Issue                | Solution                                          |
| -------------------- | ------------------------------------------------- |
| Port 8085 in use     | `netstat -ano \| findstr :8085` then kill process |
| Docker not running   | Start Docker Desktop and wait for ready           |
| DAG not visible      | `make restart` to refresh Airflow                 |
| Services won't start | `make clean` then `make all` for fresh start      |

For detailed troubleshooting, see [StartupDocs.md](StartupDocs.md).

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License.

---

**Built for data engineering learning and portfolio development**  
_Showcasing ETL pipelines, Airflow orchestration, and modern data engineering practices_
