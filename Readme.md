# ğŸ›’ Web Sales Analytics Pipeline

[![Tests](https://github.com/ibrahimGoumrane/web_sales_analytic_pipeline/actions/workflows/tests.yml/badge.svg)](https://github.com/ibrahimGoumrane/web_sales_analytic_pipeline/actions/workflows/tests.yml)
[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-ready-brightgreen.svg)](https://www.docker.com/)
[![Airflow](https://img.shields.io/badge/airflow-3.1.3-red.svg)](https://airflow.apache.org/)

> **Production-grade data engineering project** featuring automated ETL pipelines, real-time analytics, and interactive dashboards for e-commerce price monitoring.

## ğŸ“Š Overview

An end-to-end data engineering solution that **automatically scrapes, processes, and analyzes** product data from Jumia.ma (Morocco's largest e-commerce platform). The system generates comprehensive analytics reports with **visualizations** and provides an **interactive Streamlit dashboard** for real-time insights.

**Built with modern data engineering best practices:** containerization, orchestration, automated testing, and CI/CD.

## âœ¨ Key Features

### ğŸ” **Automated Web Scraping**

- Multi-category product extraction from Jumia.ma
- Intelligent pagination and rate limiting
- 1000+ products scraped daily across electronics, fashion, home goods
- Robust error handling and retry logic

### ğŸ”„ **Smart Data Processing**

- Advanced price normalization (handles multiple currency formats)
- Multi-locale numeric parsing (commas, spaces, mixed formats)
- Data quality validation and cleansing
- Idempotent transformations for reliable re-runs

### ğŸ’¾ **PostgreSQL Data Warehouse**

- Automated schema creation and migration
- Optimized indexing on SKU and website columns
- UPSERT logic for conflict resolution
- Separate databases: `airflow` (metadata) + `sales_analytics` (data)

### âš™ï¸ **Apache Airflow Orchestration**

- **Daily ETL DAG:** Automated scraping, transformation, and loading
- **Analytics DAG:** On-demand report generation with 7 CSV outputs + 6 visualizations
- Task dependencies and failure notifications
- Parameterized execution with date selection

### ğŸ“Š **Interactive Dashboard**

- Real-time Streamlit dashboard with live database connection
- Category breakdown, price trends, and top products
- Export capabilities and date range filtering
- Professional metrics and visualizations

### ğŸ§ª **Comprehensive Testing & CI/CD**

- Automated unit tests for all pipeline components
- GitHub Actions workflow with Docker Compose integration
- Code coverage reporting
- Tests run on every push/PR

## ğŸ› ï¸ Tech Stack

| Component            | Technology                  |
| -------------------- | --------------------------- |
| **Orchestration**    | Apache Airflow 3.1.3        |
| **Web Scraping**     | BeautifulSoup4, Requests    |
| **Database**         | PostgreSQL 16               |
| **Data Processing**  | Pandas, NumPy               |
| **Visualization**    | Matplotlib, Seaborn, Plotly |
| **Dashboard**        | Streamlit                   |
| **Containerization** | Docker & Docker Compose     |
| **CI/CD**            | GitHub Actions, pytest      |
| **Python**           | 3.12                        |

## ğŸ“ Project Structure

```text
web_sales_analytic_pipeline/
â”‚
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ sales_etl_dag.py          # Main DAG definition (scrape, transform, load)
â”‚
â”œâ”€â”€ scraping/
â”‚   â”œâ”€â”€ base.py                       # Abstract base scraper class
â”‚   â”œâ”€â”€ main.py                       # Scraper orchestrator
â”‚   â”œâ”€â”€ utils.py                      # URL handling utilities
â”‚   â””â”€â”€ jumia/
â”‚       â””â”€â”€ main.py                   # Jumia scraper implementation
â”‚
â”œâ”€â”€ transform/
â”‚   â”œâ”€â”€ base.py                       # Abstract base transformer
â”‚   â”œâ”€â”€ main.py                       # Transform orchestrator
â”‚   â””â”€â”€ jumia/
â”‚       â””â”€â”€ main.py                   # Jumia data cleaner
â”‚
â”œâ”€â”€ load/
â”‚   â””â”€â”€ load_postgres.py              # PostgreSQL loader with analytics queries
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ base.py                       # Analytics report generator
â”‚   â”œâ”€â”€ helpers.py                    # Plotting utility functions
â”‚   â”œâ”€â”€ main.py                       # Report orchestrator
â”‚   â””â”€â”€ __init__.py                   # Lazy import wrapper
â”‚
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ main.py                       # Streamlit dashboard app
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_scraper.py               # Scraper unit tests
â”‚   â”œâ”€â”€ test_transformer.py           # Data cleaning tests
â”‚   â”œâ”€â”€ test_loader.py                # Database loader tests
â”‚   â””â”€â”€ test_reports.py               # Report generation tests
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ tests.yml                 # CI/CD pipeline configuration
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/jumia/                    # Raw scraped data (JSON/CSV)
â”‚   â”œâ”€â”€ processed/jumia/              # Cleaned data (CSV)
â”‚   â””â”€â”€ reports/report_YYYYMMDD/      # Generated analytics reports
â”‚       â”œâ”€â”€ *.csv                     # Category stats, top products, discounts, etc.
â”‚       â”œâ”€â”€ *.png                     # Visualizations and charts
â”‚       â””â”€â”€ REPORT_SUMMARY.txt        # Executive summary
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ screenshots/                  # UI screenshots for documentation
â”‚   â””â”€â”€ samples/                      # Sample data outputs
â”‚
â”œâ”€â”€ Dockerfile                        # Custom Airflow image with plotting libs
â”œâ”€â”€ streamlit.Dockerfile              # Streamlit dashboard image
â”œâ”€â”€ pytest.Dockerfile                 # Test runner image
â”œâ”€â”€ docker-compose.yaml               # Multi-container orchestration
â”œâ”€â”€ Makefile                          # Build automation commands
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ .env                              # Environment variables
â”œâ”€â”€ StartupDocs.md                    # Quick start guide
â”œâ”€â”€ Recomendations.md                 # Enhancement roadmap
â””â”€â”€ README.md                         # This file
```

## ğŸš€ Getting Started

### Prerequisites

- **Docker Desktop** 20.10+
- **Python** 3.12+
- **Make** (optional, for convenience commands)
- **Git**

### Quick Start

1. **Clone the repository**

   ```bash
   git clone https://github.com/ibrahimGoumrane/web_sales_analytic_pipeline.git
   cd web_sales_analytic_pipeline
   ```

2. **Ensure Docker Desktop is running**

   ```bash
   docker --version
   docker ps
   ```

3. **Start the entire pipeline**

   ```bash
   make all
   ```

   This will:

   - Start PostgreSQL database
   - Initialize Airflow (creates admin user)
   - Start Airflow webserver and scheduler
   - Launch Streamlit dashboard
   - Wait ~2-3 minutes for initialization

4. **Access the dashboards**

   **Airflow UI:**

   - URL: <http://localhost:8085>
   - Username: `admin`
   - Password: `admin`

   **Streamlit Dashboard:**

   - URL: <http://localhost:8501>
   - Interactive analytics and visualizations

5. **Run the pipeline**
   - Find `jumia_daily_etl` DAG in the Airflow UI
   - Toggle switch to enable
   - Click â–¶ï¸ Play â†’ Trigger DAG
   - Monitor execution (Green = Success)

### Verify Installation

```bash
# Check services are running
make ps

# View logs
make logs

# Check database
docker exec -it web_sales_analytic_pipeline-postgres-1 psql -U airflow -d sales_analytics
# Inside psql:
SELECT COUNT(*) FROM products WHERE website = 'jumia';
\q
```

### Available Commands

```bash
make help          # Show all available commands
make up            # Start all services
make down          # Stop all services
make restart       # Restart services
make logs          # View all logs
make clean         # Remove all data and reset
make ps            # Check container status
make test          # Run unit tests locally
```

For detailed setup instructions, see [StartupDocs.md](StartupDocs.md).

## ğŸ“Š Data Output

### Database Schema

**Database:** `sales_analytics`  
**Table:** `products`

| Column            | Type          | Description            |
| ----------------- | ------------- | ---------------------- |
| id                | SERIAL        | Primary key            |
| website           | VARCHAR(50)   | Source website (jumia) |
| sku               | VARCHAR(100)  | Product SKU            |
| name              | TEXT          | Product name           |
| url               | TEXT          | Product URL            |
| current_price     | NUMERIC(10,2) | Current price          |
| old_price         | NUMERIC(10,2) | Original price         |
| discount          | NUMERIC(5,2)  | Discount percentage    |
| rating            | NUMERIC(3,2)  | Product rating (0-5)   |
| review_count      | INTEGER       | Number of reviews      |
| is_official_store | BOOLEAN       | Official store badge   |
| image_url         | TEXT          | Product image URL      |
| scraped_at        | TIMESTAMP     | Scraping timestamp     |
| created_at        | TIMESTAMP     | Record creation time   |

### File Outputs

- **`data/raw/jumia/`** â€” Raw scraped data (JSON/CSV)
- **`data/processed/jumia/`** â€” Cleaned product data (CSV)
- **`data/reports/report_YYYYMMDD/`** â€” Analytics reports and visualizations:
  - `category_statistics.csv` â€” Product counts, prices, ratings by category
  - `top_rated_products.csv` â€” Top 50 products by weighted score
  - `biggest_discounts.csv` â€” Top 100 products by discount percentage
  - `price_statistics.csv` â€” Price distribution metrics
  - `rating_statistics.csv` â€” Rating and review analysis
  - `store_performance.csv` â€” Official vs non-official store comparison
  - `daily_summary.csv` â€” Daily aggregated statistics
  - `*.png` â€” 6 visualization charts
  - `REPORT_SUMMARY.txt` â€” Executive summary
- **`logs/`** â€” Application logs

## âš™ï¸ Pipeline Execution

### Daily ETL DAG (`jumia_daily_etl`)

Runs daily at midnight with the following workflow:

1. **Scrape Jumia** (Task: `scrape_jumia`)

   - Discovers categories automatically
   - Scrapes 100 products per category
   - Saves raw data to `data/raw/jumia/`

2. **Transform Data** (Task: `transform_jumia`)

   - Cleans prices and percentages
   - Normalizes ratings and reviews
   - Saves to `data/processed/jumia/`

3. **Load to Database** (Task: `load_jumia`)
   - Inserts cleaned data into PostgreSQL
   - Handles duplicates with UNIQUE constraints
   - Creates `sales_analytics` database automatically

### Analytics Report DAG (`analytics_report_param`)

Manual/API-triggered report generation:

**Generate Reports** (Task: `generate_report`)

- Loads data from PostgreSQL by date
- Generates 7 CSV reports with statistics
- Creates 6 PNG visualizations (matplotlib/seaborn/plotly)
- Saves to `data/reports/report_YYYYMMDD/`
- Trigger with JSON: `{"website": "jumia", "date": "YYYY-MM-DD"}`

## ğŸ¯ Skills Demonstrated

- âœ… **ETL Pipeline Design** â€” Complete Extract-Transform-Load workflow
- âœ… **Apache Airflow** â€” DAG creation, task dependencies, scheduling
- âœ… **Web Scraping** â€” BeautifulSoup, pagination, dynamic content handling
- âœ… **Data Engineering** â€” Advanced data cleaning, type conversions, validation
- âœ… **Database Design** â€” PostgreSQL schema, indexing, UPSERT operations
- âœ… **Containerization** â€” Multi-service Docker Compose orchestration
- âœ… **Testing** â€” Unit tests, integration tests, CI/CD automation
- âœ… **DevOps** â€” GitHub Actions, automated testing, Docker workflows
- âœ… **Data Visualization** â€” Matplotlib, Seaborn, Plotly dashboards
- âœ… **Code Quality** â€” Abstract base classes, modular design, type hints

## ğŸš€ Future Enhancements

See [Recomendations.md](Recomendations.md) for detailed roadmap:

- ğŸŒ **REST API Layer** â€” FastAPI endpoints for programmatic access
- â˜ï¸ **Cloud Deployment** â€” AWS/GCP/Azure production deployment
- ğŸ›ï¸ **Multi-Site Support** â€” Expand to other e-commerce platforms
- ğŸ¤– **ML Price Prediction** â€” Forecast price trends using historical data
- ğŸ“§ **Alert System** â€” Email notifications for price drops and deals

## ğŸ”— Access Points

### Airflow Web UI

- **URL:** <http://localhost:8085>
- **Username:** `admin`
- **Password:** `admin`

### Streamlit Dashboard

- **URL:** <http://localhost:8501>
- **Features:** Interactive analytics, filters, exports

### PostgreSQL Database

- **Host:** `localhost`
- **Port:** `5432`
- **User:** `airflow`
- **Password:** `airflow`
- **Databases:** `airflow`, `sales_analytics`

### Running Tests

```bash
# Local test execution
docker compose run --rm tests

# CI/CD runs automatically on push to main/develop
```

## ğŸ› Troubleshooting

Common issues and solutions:

| Issue                | Solution                                          |
| -------------------- | ------------------------------------------------- |
| Port 8085 in use     | `netstat -ano \| findstr :8085` then kill process |
| Docker not running   | Start Docker Desktop and wait for ready           |
| DAG not visible      | `make restart` to refresh Airflow                 |
| Services won't start | `make clean` then `make all` for fresh start      |

For detailed troubleshooting, see [StartupDocs.md](StartupDocs.md).

## ğŸ“¸ Screenshots

### Airflow DAGs

![Airflow ETL DAG](docs/screenshots/dag_jumia_scraper.png)
![Analytics Report DAG](docs/screenshots/dag_report_generation.png)

### Streamlit Dashboard

![Dashboard](docs/screenshots/streamlit.png)

### Database

![PostgreSQL](docs/screenshots/pgdb.png)

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ“§ Contact

**Ibrahim Goumrane**

- GitHub: [@ibrahimGoumrane](https://github.com/ibrahimGoumrane)
- LinkedIn: [Ibrahim Goumrane](https://www.linkedin.com/in/ibrahim-goumrane)

---

<div align="center">

**â­ Star this repo if you find it useful!**

_Built to showcase modern data engineering practices and production-ready ETL pipelines_

</div>
